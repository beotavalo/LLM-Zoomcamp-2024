{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd1eaa8-3424-41ad-9cf2-3e8548712865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8180e7e4-b90d-4900-a59b-d22e5d6537c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3c2dd7-f64a-4dc7-a4e3-3e8aadfa720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_documents = {\n",
    "    'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94efe26-05e8-4ae5-a0fa-0a8e16852816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm-zoomcamp\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course, file_id in faq_documents.items():\n",
    "    print(course)\n",
    "    course_documents = read_faq(file_id)\n",
    "    documents.append({'course': course, 'documents': course_documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f08e9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "465198a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eeca0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 'llm-zoomcamp',\n",
       "  'documents': [{'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I just discovered the course. Can I still join?'},\n",
       "   {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?'},\n",
       "   {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?'},\n",
       "   {'text': 'Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\\nIssue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\\nAnswer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How do I get access?'},\n",
       "   {'text': 'We get 15 free hours per month, which might be limited to the free tier’s hardware configuration.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How many free hours do we get?'},\n",
       "   {'text': 'This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month'},\n",
       "   {'text': 'Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\\nGoogle Colab\\nKaggle\\nDatabricks (?), so many others.\\nUse GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Cloud alternatives with GPU'},\n",
       "   {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?'},\n",
       "   {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?'},\n",
       "   {'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I missed the first homework - can I still get a certificate?'},\n",
       "   {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I was working on next week’s homework/content - why does it keep changing?'},\n",
       "   {'text': 'Summer 2025 (via Alexey).',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'When will the course be offered next?'},\n",
       "   {'text': 'Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Are there any lectures/videos? Where are they?'},\n",
       "   {'text': 'Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\\\Users\\\\YourUsername\\\\.wslconfig) with the desired RAM allocation:\\n[wsl2]\\nmemory=8GB\\nRestart WSL: wsl --shutdown\\nRun the free command to verify the changes. For more details, read this article.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.'},\n",
       "   {'text': 'You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error when running OpenAI chat.completions.create command'},\n",
       "   {'text': \"RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\\nThe above errors are related to your OpenAI API account’s quota.\\nThere is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\\nSteps to resolve:\\nAdd credits to your account here (min $5)\\nIn chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\\nYou might need to recreate an API key after adding credits to your account and update it locally.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error: RateLimitError: Error code: 429 -'},\n",
       "   {'text': 'Update openai version from 0.27.0 -> any 1.x version',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\"},\n",
       "   {'text': 'Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: How much will I have to spend to use the Open AI API?'},\n",
       "   {'text': \"No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\\nllm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Do I have to subscribe and pay for Open AI API for this course?'},\n",
       "   {'text': 'If you get this error, it’s likely that elasticsearch doesn’t get enough RAM\\nI specified the RAM size to the configuration (-m 4GB)\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-m 4GB \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nOr give it _less_ RAM:\\nTip for Github Codespace users\\nIf you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\\nIn that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\\nThis reduces the resource usage.\\nFull command:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nIf it doesn\\'t work, try this:\\nsudo sysctl -w vm.max_map_count=262144\\nAnd give the Java machine inside the container more RAM:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n--ulimit nofile=65536:65536 \\\\\\n--ulimit memlock=-1:-1 \\\\\\n--memory=4g \\\\\\n--cpus=2 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnother possible solution may be to set the memory_lock to false:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\n-e \"bootstrap.memory_lock=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'ElasticSearch: ERROR: Elasticsearch exited unexpectedly'},\n",
       "   {'text': 'Instead of document as used in the course video, use doc',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   {'text': 'When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\\ndocker volume create elasticsearch_data\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-v elasticsearch_data:/usr/share/elasticsearch/data \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Docker: How do I store data persistently in Elasticsearch?'},\n",
       "   {'text': \"You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\\nFor example, you can create a new file named “api_keys.yml” in your repository.\\nThen, do not forget to add it in your .gitignore file:\\n#api_keys\\napi_keys.yml\\nYou can now fill your api_keys.yml file:\\nOPENAI_API_KEY: “sk[...]”\\nGROQ_API_KEY: “gqk_[...]”\\nSave your file.\\nYou will need the pyyaml library to load your yaml file, so run this command in your terminal:\\npip install pyyaml\\nNow, open your jupyter notebook.\\nYou can load your yaml file and the associated keys with this code:\\nimport yaml\\n# Open the file\\nwith open('api_keys.yml', 'r') as file:\\n# Load the data from the file\\ndata = yaml.safe_load(file)\\n# Get the API key (Groq example here)\\ngroq_api_key = data['GROQ_API_KEY']\\nNow, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\\nAdded by Mélanie Fouesnard\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Safe and easy way to store and load API keys'},\n",
       "   {'text': 'Option1: using direnv\\ncreated the .envrc file & added my API key, ran direnv allow in the terminal\\nwas getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\\nresolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\\nfrom dotenv import load_dotenv\\nload_dotenv(\\'.envrc\\')\\nOption 2: using Codespaces Secrets\\nLog in to your GitHub account and navigate to Settings > Codespaces\\nThere is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\\nOnce you set this up, the key will be available in your codespaces session',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?'},\n",
       "   {'text': 'Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\\nTo download ollama for Ubuntu:\\n``` curl -fsSL https://ollama.com/install.sh | sh ```\\nTo download ollama for Mac and Windows, follow the guide on this link:\\nhttps://ollama.com/download/\\nOllama a number of open-source LLMs like:\\nLlama3\\nPhi3\\nMistral and Mixtral\\nGemma\\nQwen\\nYou can explore more models on https://ollama.com/library/\\nTo download a model in Ollama, simply open command prompt and type:\\n``` ollama run model_name ```\\ne.g.\\n``` ollama run phi3 ```\\nIt will automatically download the model and you can use it same way as above for later time.\\nTo use Ollama models for inference and llm-zoomcamp tasks, use the following function:\\nimport ollama\\ndef llm(prompt):\\nresponse = ollama.chat(\\nmodel=\"llama3\",\\nmessages=[{\"role\": \"user\", \"content\": prompt}]\\n)\\nreturn response[\\'message\\'][\\'content\\']\\nFor example, we can use it in the following way:\\nprompt = \"When does the llm-zoomcamp course start?\"\\nanswer = llm(prompt)\\nprint(answer)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: How can I use Ollama open-source models locally on my pc without using any API?'},\n",
       "   {'text': \"The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\"},\n",
       "   {'text': 'You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use Groq instead of OpenAI?'},\n",
       "   {'text': 'Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use open-source alternatives to OpenAI API?'},\n",
       "   {'text': 'This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Returning Empty list after filtering my query (HW Q3)'},\n",
       "   {'text': 'Answer',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Question'},\n",
       "   {'text': 'Please see the General section or use CTRL+F to search this doc.',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Saturn Cloud issues'},\n",
       "   {'text': 'Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\\nOnce you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\\n1- Navigate to Your Project Directory:\\ncd /home/jovyan/my_project\\n2- Configure GitHub Remote to Use SSH:\\ngit remote set-url origin git@github.com:username/repository.git\\n3- Stage, Commit and push your changes:\\ngit add .\\ngit commit -m \"Your commit message\"\\ngit push',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?'},\n",
       "   {'text': 'Clean out your cache using the following code:\\nfrom transformers import TRANSFORMERS_CACHE\\nprint(TRANSFORMERS_CACHE)\\nimport shutil\\nshutil.rmtree(TRANSFORMERS_CACHE)\\nNote: Make sure to shutdown the notebook and restart the kernel',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?'},\n",
       "   {'text': 'Yes, you can. Here the step to follow:\\n- Open a bash session in the elasticsearch container\\n```bash\\ndocker exec -it elasticsearch bash\\n```\\n- Add path.repo configuration:\\n```bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart container and verify it was created correctly:\\n```bash\\ndocker restart elasticsearch\\ncurl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\\n```\\n- Create the snapshot (this is the backup ;) )\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Copy the backup to my machine:\\n```bash\\ndocker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\\n```\\n- Now create the new container or use docker-compose just in case you are following the module 2:\\n```bash\\ndocker compose up -d\\n```\\n- Add de path.repo configuration in the new one, same as before:\\n```bash\\ndocker exec -it new_elasticsearch bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart the docker container and copy the snapshot in it:\\n```bash\\ndocker restart new_elasticsearch\\ndocker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\\n```\\n- Register the Snapshot Repository in the New Container:\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"type\": \"fs\",\\n\"settings\": {\\n\"location\": \"/usr/share/elasticsearch/backup\"\\n}\\n}\\n\\'\\n```\\n- Verify if it exists:\\n```bash\\ncurl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\\n```\\n- Restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Show your indexes:\\n```bash\\ncurl -X GET \"localhost:9200/_cat/indices?v\"\\n```\\n- Extra point: If you want to change the original index name by other when you restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"old_index\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false,\\n\"rename_pattern\": \"old_index\",\\n\"rename_replacement\": \"new_index\"\\n}\\n\\'\\n```',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?'},\n",
       "   {'text': 'You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\\n- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\\n- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\ncpus: \\'1.0\\'  # Limits to 1 CPU\\nreservations:\\ncpus: \\'0.5\\'  # Reserves 0.5 CPUs',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: How can I limit the memory used by the ElasticSearch container?'},\n",
       "   {'text': 'You have several ways to inspect the content of a file when you are inside a Docker container.\\nFirst, make sure you ran the docker container interactively using bash:\\ndocker exec -it <container> bash\\nThen, you are able to use bash commands. For this case, I propose two solutions:\\nUse “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\\nInstall vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\\napt-get install vim\\nvim your_file\\nThen, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How to inspect the content of a file inside a Docker container ?'},\n",
       "   {'text': 'Use the following line instead in mounting the current volume to docker for Q4:\\n`-v \"/${PWD}/ollama_files:/root/.ollama\"`',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Error: Docker mounted volume adds ;C to end of windows path'},\n",
       "   {'text': 'In Docker Desktop, try to increase the resource.\\nGo to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\\nAdded by Dandy Arif Rahman',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?'},\n",
       "   {'text': 'docker system prune -a',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How can to clean docker cache?'},\n",
       "   {'text': 'A network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\\nAdded by Eduardo Muñoz',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama'},\n",
       "   {'text': 'To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\\nAdded by Taras Goriachko\\nOllama: Running Ollama locally on Colab gives error after the llm() line\\nAPIConnectionError: Connection error.\\nIt seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\\nFound a solution in the Medium article and this link:\\nhttps://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\\nhttps://github.com/ollama/ollama/issues/703\\nAdded by Hanaa',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error: NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \"model XXX not found, try pulling it first\" …'},\n",
       "   {'text': 'ollama list\\nollama rm [model_name]',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: How can remove Ollama model?'},\n",
       "   {'text': 'InternalServerError: Error code: 500 - {\\'error\\': {\\'message\\': \\'model requires more system memory (5.6 GiB) than is available (1.5 GiB)\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}.\\nRunning elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\\nversion: \\'3.8\\'\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\n- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\nmemory: 2G  # change 2\\nollama:\\nimage: ollama/ollama\\ncontainer_name: ollama\\nvolumes:\\n- ollama:/root/.ollama\\nports:\\n- \"11434:11434\"\\nvolumes:\\nollama:\\nAdded by Zoe Zelkha',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error code 500 InternalServerError'},\n",
       "   {'text': 'Manually set the token as below:\\naccess_token = <your_token>\\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF'},\n",
       "   {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': \"Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\"},\n",
       "   {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3'},\n",
       "   {'text': 'pip install protobuf==3.20.1\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: from google.protobuf.pyext import _message / TypeError: bases must be types'},\n",
       "   {'text': '1. search with the model name on hugging face.\\n2. get the transformer used on the model.\\n3. using the transformer, encode the string you want.\\n4. calculate the length of the outputted tensor.\\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM. \\nDon’t forget to make your token secret.\\nAdded by kamal',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?'},\n",
       "   {'text': 'The last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\\nIn my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\\nTo install all the needed packages use this command:\\n!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\\nAnd after that just executed this command:\\n!pip install --upgrade transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'How to run a model using CUDA for GPU usage?'},\n",
       "   {'text': 'Upgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\\nOr use the keyword ‘body’ instead of ‘document’\\nFor conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\\n\\n$ conda config --add channels conda-forge\\n$ conda config --set channel_priority strict\\n$ conda install -c conda-forge elasticsearch==8.4.3',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   {'text': 'This worked for me:',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\"},\n",
       "   {'text': 'Try to running docker container based on first course module like this :\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnd don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'ElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search'},\n",
       "   {'text': 'As seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\\nThe KNN related score, which is between -1 and 1 (cosine similarity)\\nThe text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document\\'s relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\\nSince we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\\nTo get more details about the final scores, you can modify the search query and add an “explain” parameter:\\nresponse = es_client.search(\\nindex=index_name,\\nquery={\\n\"match\": {\"section\": \"General course-related questions\"},\\n},\\nknn=knn_query,\\nsize=5,\\nexplain=True\\n)\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Why do I get scores greater than 1 on my hits after querying my ElasticSearch database ?'},\n",
       "   {'text': 'For this module homework make sure you install the package sentence-transformers it can be installed as simply as:\\npip install sentence-transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Not module named “sentence_transformers”'},\n",
       "   {'text': 'I was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\\nI checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\\nMy problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\\ndocker image prune\\ndocker volume prune\\ndocker container prune\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Can not create the index: Connection timeout.'},\n",
       "   {'text': 'Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\\nv_query = embedding_model.encode(query_text)\\nresults = search_function(v_query)',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\"},\n",
       "   {'text': 'max_value = numpy_array.max()',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Find maximum of an numpy array (of any dimension):'},\n",
       "   {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What is the cosine similarity?'},\n",
       "   {'text': 'A “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What are documents in ElasticSearch?'},\n",
       "   {'text': 'docker stop elasticsearch\\ndocker rm elasticsearch\\nHow to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\\nscore = (es_score - 0.5) * 2',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'runing docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See \\'docker run --help\\'.'},\n",
       "   {'text': 'Upgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'Warning: \\'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet\\' how to suppress?'},\n",
       "   {'text': 'Solution 1 : Install Visual C++ Redistributable\\nSolution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'In Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\lib\\\\fbgemm.dll\" or one of its dependencies.'},\n",
       "   {'text': 'Inside .env file change POSTGRES_HOST=localhost',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?'},\n",
       "   {'text': 'By default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\\nThe instruction to show the results must be preceded by:\\npd.set_option(\\'display.max_colwidth\\', None)\\nHere are the specs for the display_max_colwidth option, as describide in the official docs:\\ndisplay.max_colwidth : int or None\\nThe maximum width in characters of a column in the repr of\\na pandas data structure. When the column overflows, a \"...\"\\nplaceholder is embedded in the output. A \\'None\\' value means unlimited.\\n[default: 50] [currently: 50]',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook'},\n",
       "   {'text': 'import numpy as np\\nnormalize_vec = lambda v: v / np.linalg.norm(v)\\ndf[\"new_col\"] = df[\"org_col\"].apply(norm_vec)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?'},\n",
       "   {'text': 'To compute the 75% percentile or 0.75 quantile:\\nquantile: int = df[\"col\"].quantile(q=0.75)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?'},\n",
       "   {'text': '1. Delete all containers (including running ones):\\n```\\ndocker rm -f\\n```\\n2. Remove all images:\\n```\\ndocker rmi -f\\n```\\n3. Delete all volumes:\\n```\\ndocker volume rm\\n```',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': 'How can I remove all Docker containers, images, and volumes, and builds from the terminal?'},\n",
       "   {'text': 'Use the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': \"I have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\"},\n",
       "   {'text': 'Answer', 'section': 'Module 6: X', 'question': 'Question'},\n",
       "   {'text': 'Answer', 'section': 'Module 6: X', 'question': 'Question'},\n",
       "   {'text': 'Answer', 'section': 'Capstone Project', 'question': 'Question'},\n",
       "   {'text': 'No, the capstone is a solo project.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Is it a group project?'},\n",
       "   {'text': 'You only need to submit 1 project. \\nIf the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\\nIf you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\\nIf you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Do we submit 2 projects, what does attempt 1 and 2 mean?'},\n",
       "   {'text': 'No, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Does the competition count as the capstone?'},\n",
       "   {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'How is my capstone project going to be evaluated?'},\n",
       "   {'text': 'Answer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.',\n",
       "    'section': 'Certificates',\n",
       "    'question': 'Do I have to use ElasticSearch or X library?'},\n",
       "   {'text': 'Answer', 'section': 'Workshops: dlthub', 'question': 'Question'},\n",
       "   {'text': 'Since dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Can I use the workshop materials for my own projects or share them with others?'},\n",
       "   {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   {'text': 'You can use the db.table_names() to list all the tables in the db',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'How do I know which tables are in the db'},\n",
       "   {'text': 'Currently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Does DLT have connectors to ClickHouse or StarRocks?'},\n",
       "   {'text': 'If you get this error\\nOr 401 Client Error , then you either need to grant access to the key or the key is wrong.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Notebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search'},\n",
       "   {'text': 'Install directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?'},\n",
       "   {'text': 'If you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\\nIf you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\\nAdded by Abiodun Gbadamosi',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Connection refused error on prompting the ollam RAG?'},\n",
       "   {'text': 'Answer', 'section': 'Workshops: X', 'question': 'Question'}]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0209c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b71613b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dict_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f72995ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86 entries, 0 to 85\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   course     86 non-null     object\n",
      " 1   documents  86 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "089e82f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c02fe3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "\n",
    "def chunk_documents(data: List[Dict[str, Any]], *args, **kwargs):\n",
    "    documents = []\n",
    "    \n",
    "    for idx, item in enumerate(data):\n",
    "        course = item['course']\n",
    "        \n",
    "        for info in item['documents']:\n",
    "            section = info['section']\n",
    "            question = info['question']\n",
    "            answer = info['text']\n",
    "            \n",
    "            # Generate a unique document ID\n",
    "            document_id = ':'.join([re.sub(r'\\W', '_', part) \n",
    "\t            for part in [course, section, question]]).lower()\n",
    "            \n",
    "            # Format the document string\n",
    "            chunk = '\\n'.join([\n",
    "                f'course:\\n{course}\\n',\n",
    "                f'section:\\n{section}\\n',\n",
    "                f'question:\\n{question}\\n',\n",
    "                f'answer:\\n{answer}\\n',\n",
    "            ])\n",
    "            \n",
    "            documents.append(dict(\n",
    "                chunk=chunk,\n",
    "                document=info,\n",
    "\t            document_id=document_id,\n",
    "            ))\n",
    "\n",
    "    print(f'Documents:', len(documents))\n",
    "            \n",
    "    return [documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb5c769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 86\n"
     ]
    }
   ],
   "source": [
    "doc_chunks = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fa3c3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f16a9048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nI just discovered the course. Can I still join?\\n\\nanswer:\\nYes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.\\n',\n",
       "   'document': {'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I just discovered the course. Can I still join?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:i_just_discovered_the_course__can_i_still_join_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nCourse - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?\\n\\nanswer:\\nYou don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\\n\",\n",
       "   'document': {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:course___i_have_registered_for_the__insert_zoomcamp_name___when_can_i_expect_to_receive_the_confirmation_email_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nWhat is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?\\n\\nanswer:\\nThe zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.\\n',\n",
       "   'document': {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:what_is_the_video_zoom_link_to_the_stream_for_the__office_hours__or_live_workshop_sessions_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nSaturnCloud - How do I get access?\\n\\nanswer:\\nIssue: I get the notice that due to traffic, I’m on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\\nIssue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\\nAnswer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .\\n',\n",
       "   'document': {'text': 'Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\\nIssue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\\nAnswer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How do I get access?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:saturncloud___how_do_i_get_access_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nSaturnCloud - How many free hours do we get?\\n\\nanswer:\\nWe get 15 free hours per month, which might be limited to the free tier’s hardware configuration.\\n',\n",
       "   'document': {'text': 'We get 15 free hours per month, which might be limited to the free tier’s hardware configuration.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How many free hours do we get?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:saturncloud___how_many_free_hours_do_we_get_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nSaturnCloud - Something went wrong. Max of 15 hours of resource usage per month\\n\\nanswer:\\nThis message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.\\n',\n",
       "   'document': {'text': 'This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:saturncloud___something_went_wrong__max_of_15_hours_of_resource_usage_per_month'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nCloud alternatives with GPU\\n\\nanswer:\\nCheck the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\\nGoogle Colab\\nKaggle\\nDatabricks (?), so many others.\\nUse GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.\\n',\n",
       "   'document': {'text': 'Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\\nGoogle Colab\\nKaggle\\nDatabricks (?), so many others.\\nUse GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Cloud alternatives with GPU'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:cloud_alternatives_with_gpu'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nLeaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\\n\\nanswer:\\nWhen you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!\\n',\n",
       "   'document': {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:leaderboard___i_am_not_on_the_leaderboard___how_do_i_know_which_one_i_am_on_the_leaderboard_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nCertificate - Can I follow the course in a self-paced mode and get a certificate?\\n\\nanswer:\\nNo, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\\n\",\n",
       "   'document': {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:certificate___can_i_follow_the_course_in_a_self_paced_mode_and_get_a_certificate_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nI missed the first homework - can I still get a certificate?\\n\\nanswer:\\nYes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.\\n',\n",
       "   'document': {'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I missed the first homework - can I still get a certificate?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:i_missed_the_first_homework___can_i_still_get_a_certificate_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nI was working on next week’s homework/content - why does it keep changing?\\n\\nanswer:\\nThis course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.\\n',\n",
       "   'document': {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I was working on next week’s homework/content - why does it keep changing?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:i_was_working_on_next_week_s_homework_content___why_does_it_keep_changing_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nWhen will the course be offered next?\\n\\nanswer:\\nSummer 2025 (via Alexey).\\n',\n",
       "   'document': {'text': 'Summer 2025 (via Alexey).',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'When will the course be offered next?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:when_will_the_course_be_offered_next_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nGeneral course-related questions\\n\\nquestion:\\nAre there any lectures/videos? Where are they?\\n\\nanswer:\\nPlease check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.\\n',\n",
       "   'document': {'text': 'Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Are there any lectures/videos? Where are they?'},\n",
       "   'document_id': 'llm_zoomcamp:general_course_related_questions:are_there_any_lectures_videos__where_are_they_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nWSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.\\n\\nanswer:\\nYour WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\\\Users\\\\YourUsername\\\\.wslconfig) with the desired RAM allocation:\\n[wsl2]\\nmemory=8GB\\nRestart WSL: wsl --shutdown\\nRun the free command to verify the changes. For more details, read this article.\\n',\n",
       "   'document': {'text': 'Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\\\Users\\\\YourUsername\\\\.wslconfig) with the desired RAM allocation:\\n[wsl2]\\nmemory=8GB\\nRestart WSL: wsl --shutdown\\nRun the free command to verify the changes. For more details, read this article.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:wsl2___responseerror__model_requires_more_system_memory__x_x_gib__than_is_available__y_y_gib___my_system_has_more_than_x_x_gib_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenAI: Error when running OpenAI chat.completions.create command\\n\\nanswer:\\nYou may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:\\n',\n",
       "   'document': {'text': 'You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error when running OpenAI chat.completions.create command'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:openai__error_when_running_openai_chat_completions_create_command'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenAI: Error: RateLimitError: Error code: 429 -\\n\\nanswer:\\nRateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\\nThe above errors are related to your OpenAI API account’s quota.\\nThere is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\\nSteps to resolve:\\nAdd credits to your account here (min $5)\\nIn chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\\nYou might need to recreate an API key after adding credits to your account and update it locally.\\n\",\n",
       "   'document': {'text': \"RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\\nThe above errors are related to your OpenAI API account’s quota.\\nThere is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\\nSteps to resolve:\\nAdd credits to your account here (min $5)\\nIn chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\\nYou might need to recreate an API key after adding credits to your account and update it locally.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error: RateLimitError: Error code: 429 -'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:openai__error__ratelimiterror__error_code__429__'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\\n\\nanswer:\\nUpdate openai version from 0.27.0 -> any 1.x version\\n\",\n",
       "   'document': {'text': 'Update openai version from 0.27.0 -> any 1.x version',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\"},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:openai__error___cannot_import_name_openai_from_openai___how_to_fix_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenAI: How much will I have to spend to use the Open AI API?\\n\\nanswer:\\nUsing the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.\\n',\n",
       "   'document': {'text': 'Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: How much will I have to spend to use the Open AI API?'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:openai__how_much_will_i_have_to_spend_to_use_the_open_ai_api_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenAI: Do I have to subscribe and pay for Open AI API for this course?\\n\\nanswer:\\nNo, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\\nllm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)\\n\",\n",
       "   'document': {'text': \"No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\\nllm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Do I have to subscribe and pay for Open AI API for this course?'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:openai__do_i_have_to_subscribe_and_pay_for_open_ai_api_for_this_course_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nElasticSearch: ERROR: Elasticsearch exited unexpectedly\\n\\nanswer:\\nIf you get this error, it’s likely that elasticsearch doesn’t get enough RAM\\nI specified the RAM size to the configuration (-m 4GB)\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-m 4GB \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nOr give it _less_ RAM:\\nTip for Github Codespace users\\nIf you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\\nIn that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\\nThis reduces the resource usage.\\nFull command:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nIf it doesn\\'t work, try this:\\nsudo sysctl -w vm.max_map_count=262144\\nAnd give the Java machine inside the container more RAM:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n--ulimit nofile=65536:65536 \\\\\\n--ulimit memlock=-1:-1 \\\\\\n--memory=4g \\\\\\n--cpus=2 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnother possible solution may be to set the memory_lock to false:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\n-e \"bootstrap.memory_lock=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\n',\n",
       "   'document': {'text': 'If you get this error, it’s likely that elasticsearch doesn’t get enough RAM\\nI specified the RAM size to the configuration (-m 4GB)\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-m 4GB \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nOr give it _less_ RAM:\\nTip for Github Codespace users\\nIf you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\\nIn that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\\nThis reduces the resource usage.\\nFull command:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nIf it doesn\\'t work, try this:\\nsudo sysctl -w vm.max_map_count=262144\\nAnd give the Java machine inside the container more RAM:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n--ulimit nofile=65536:65536 \\\\\\n--ulimit memlock=-1:-1 \\\\\\n--memory=4g \\\\\\n--cpus=2 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnother possible solution may be to set the memory_lock to false:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\n-e \"bootstrap.memory_lock=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'ElasticSearch: ERROR: Elasticsearch exited unexpectedly'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:elasticsearch__error__elasticsearch_exited_unexpectedly'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\\n\\nanswer:\\nInstead of document as used in the course video, use doc\\n\",\n",
       "   'document': {'text': 'Instead of document as used in the course video, use doc',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:elasticsearch__error__elasticsearch_index___got_an_unexpected_keyword_argument__document_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nDocker: How do I store data persistently in Elasticsearch?\\n\\nanswer:\\nWhen you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\\ndocker volume create elasticsearch_data\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-v elasticsearch_data:/usr/share/elasticsearch/data \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\n',\n",
       "   'document': {'text': 'When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\\ndocker volume create elasticsearch_data\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-v elasticsearch_data:/usr/share/elasticsearch/data \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Docker: How do I store data persistently in Elasticsearch?'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:docker__how_do_i_store_data_persistently_in_elasticsearch_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nAuthentication: Safe and easy way to store and load API keys\\n\\nanswer:\\nYou can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\\nFor example, you can create a new file named “api_keys.yml” in your repository.\\nThen, do not forget to add it in your .gitignore file:\\n#api_keys\\napi_keys.yml\\nYou can now fill your api_keys.yml file:\\nOPENAI_API_KEY: “sk[...]”\\nGROQ_API_KEY: “gqk_[...]”\\nSave your file.\\nYou will need the pyyaml library to load your yaml file, so run this command in your terminal:\\npip install pyyaml\\nNow, open your jupyter notebook.\\nYou can load your yaml file and the associated keys with this code:\\nimport yaml\\n# Open the file\\nwith open('api_keys.yml', 'r') as file:\\n# Load the data from the file\\ndata = yaml.safe_load(file)\\n# Get the API key (Groq example here)\\ngroq_api_key = data['GROQ_API_KEY']\\nNow, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\\nAdded by Mélanie Fouesnard\\n\",\n",
       "   'document': {'text': \"You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\\nFor example, you can create a new file named “api_keys.yml” in your repository.\\nThen, do not forget to add it in your .gitignore file:\\n#api_keys\\napi_keys.yml\\nYou can now fill your api_keys.yml file:\\nOPENAI_API_KEY: “sk[...]”\\nGROQ_API_KEY: “gqk_[...]”\\nSave your file.\\nYou will need the pyyaml library to load your yaml file, so run this command in your terminal:\\npip install pyyaml\\nNow, open your jupyter notebook.\\nYou can load your yaml file and the associated keys with this code:\\nimport yaml\\n# Open the file\\nwith open('api_keys.yml', 'r') as file:\\n# Load the data from the file\\ndata = yaml.safe_load(file)\\n# Get the API key (Groq example here)\\ngroq_api_key = data['GROQ_API_KEY']\\nNow, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\\nAdded by Mélanie Fouesnard\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Safe and easy way to store and load API keys'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:authentication__safe_and_easy_way_to_store_and_load_api_keys'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nAuthentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?\\n\\nanswer:\\nOption1: using direnv\\ncreated the .envrc file & added my API key, ran direnv allow in the terminal\\nwas getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\\nresolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\\nfrom dotenv import load_dotenv\\nload_dotenv(\\'.envrc\\')\\nOption 2: using Codespaces Secrets\\nLog in to your GitHub account and navigate to Settings > Codespaces\\nThere is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\\nOnce you set this up, the key will be available in your codespaces session\\n',\n",
       "   'document': {'text': 'Option1: using direnv\\ncreated the .envrc file & added my API key, ran direnv allow in the terminal\\nwas getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\\nresolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\\nfrom dotenv import load_dotenv\\nload_dotenv(\\'.envrc\\')\\nOption 2: using Codespaces Secrets\\nLog in to your GitHub account and navigate to Settings > Codespaces\\nThere is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\\nOnce you set this up, the key will be available in your codespaces session',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:authentication__why_is_my_openai_api_key_not_found_in_the_jupyter_notebook_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenSource: How can I use Ollama open-source models locally on my pc without using any API?\\n\\nanswer:\\nPrior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\\nTo download ollama for Ubuntu:\\n``` curl -fsSL https://ollama.com/install.sh | sh ```\\nTo download ollama for Mac and Windows, follow the guide on this link:\\nhttps://ollama.com/download/\\nOllama a number of open-source LLMs like:\\nLlama3\\nPhi3\\nMistral and Mixtral\\nGemma\\nQwen\\nYou can explore more models on https://ollama.com/library/\\nTo download a model in Ollama, simply open command prompt and type:\\n``` ollama run model_name ```\\ne.g.\\n``` ollama run phi3 ```\\nIt will automatically download the model and you can use it same way as above for later time.\\nTo use Ollama models for inference and llm-zoomcamp tasks, use the following function:\\nimport ollama\\ndef llm(prompt):\\nresponse = ollama.chat(\\nmodel=\"llama3\",\\nmessages=[{\"role\": \"user\", \"content\": prompt}]\\n)\\nreturn response[\\'message\\'][\\'content\\']\\nFor example, we can use it in the following way:\\nprompt = \"When does the llm-zoomcamp course start?\"\\nanswer = llm(prompt)\\nprint(answer)\\n',\n",
       "   'document': {'text': 'Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\\nTo download ollama for Ubuntu:\\n``` curl -fsSL https://ollama.com/install.sh | sh ```\\nTo download ollama for Mac and Windows, follow the guide on this link:\\nhttps://ollama.com/download/\\nOllama a number of open-source LLMs like:\\nLlama3\\nPhi3\\nMistral and Mixtral\\nGemma\\nQwen\\nYou can explore more models on https://ollama.com/library/\\nTo download a model in Ollama, simply open command prompt and type:\\n``` ollama run model_name ```\\ne.g.\\n``` ollama run phi3 ```\\nIt will automatically download the model and you can use it same way as above for later time.\\nTo use Ollama models for inference and llm-zoomcamp tasks, use the following function:\\nimport ollama\\ndef llm(prompt):\\nresponse = ollama.chat(\\nmodel=\"llama3\",\\nmessages=[{\"role\": \"user\", \"content\": prompt}]\\n)\\nreturn response[\\'message\\'][\\'content\\']\\nFor example, we can use it in the following way:\\nprompt = \"When does the llm-zoomcamp course start?\"\\nanswer = llm(prompt)\\nprint(answer)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: How can I use Ollama open-source models locally on my pc without using any API?'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:opensource__how_can_i_use_ollama_open_source_models_locally_on_my_pc_without_using_any_api_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\\n\\nanswer:\\nThe question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.\\n\",\n",
       "   'document': {'text': \"The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\"},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:opensource__i_am_using_groq__and_it_doesn_t_provide_a_tokenizer_library_based_on_my_research__how_can_we_estimate_the_number_of_openai_tokens_asked_in_homework_question_6_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenSource: Can I use Groq instead of OpenAI?\\n\\nanswer:\\nYou can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.\\n',\n",
       "   'document': {'text': 'You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use Groq instead of OpenAI?'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:opensource__can_i_use_groq_instead_of_openai_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nOpenSource: Can I use open-source alternatives to OpenAI API?\\n\\nanswer:\\nYes. See module 2 and the open-ai-alternatives.md in module 1 folder.\\n',\n",
       "   'document': {'text': 'Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use open-source alternatives to OpenAI API?'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:opensource__can_i_use_open_source_alternatives_to_openai_api_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 1: Introduction\\n\\nquestion:\\nReturning Empty list after filtering my query (HW Q3)\\n\\nanswer:\\nThis is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.\\n',\n",
       "   'document': {'text': 'This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Returning Empty list after filtering my query (HW Q3)'},\n",
       "   'document_id': 'llm_zoomcamp:module_1__introduction:returning_empty_list_after_filtering_my_query__hw_q3_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nQuestion\\n\\nanswer:\\nAnswer\\n',\n",
       "   'document': {'text': 'Answer',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Question'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:question'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nSaturn Cloud issues\\n\\nanswer:\\nPlease see the General section or use CTRL+F to search this doc.\\n',\n",
       "   'document': {'text': 'Please see the General section or use CTRL+F to search this doc.',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Saturn Cloud issues'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:saturn_cloud_issues'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nSaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?\\n\\nanswer:\\nOf course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\\nOnce you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\\n1- Navigate to Your Project Directory:\\ncd /home/jovyan/my_project\\n2- Configure GitHub Remote to Use SSH:\\ngit remote set-url origin git@github.com:username/repository.git\\n3- Stage, Commit and push your changes:\\ngit add .\\ngit commit -m \"Your commit message\"\\ngit push\\n',\n",
       "   'document': {'text': 'Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\\nOnce you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\\n1- Navigate to Your Project Directory:\\ncd /home/jovyan/my_project\\n2- Configure GitHub Remote to Use SSH:\\ngit remote set-url origin git@github.com:username/repository.git\\n3- Stage, Commit and push your changes:\\ngit add .\\ngit commit -m \"Your commit message\"\\ngit push',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:saturncloud__how_do_you_manage_the_changes_from_saturncloud_to_your_github_repository_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nSaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?\\n\\nanswer:\\nClean out your cache using the following code:\\nfrom transformers import TRANSFORMERS_CACHE\\nprint(TRANSFORMERS_CACHE)\\nimport shutil\\nshutil.rmtree(TRANSFORMERS_CACHE)\\nNote: Make sure to shutdown the notebook and restart the kernel\\n',\n",
       "   'document': {'text': 'Clean out your cache using the following code:\\nfrom transformers import TRANSFORMERS_CACHE\\nprint(TRANSFORMERS_CACHE)\\nimport shutil\\nshutil.rmtree(TRANSFORMERS_CACHE)\\nNote: Make sure to shutdown the notebook and restart the kernel',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:saturncloud__how_can_i_clean_out_the_hugging_face_model_cache_on_a_saturn_cloud_notebook_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?\\n\\nanswer:\\nYes, you can. Here the step to follow:\\n- Open a bash session in the elasticsearch container\\n```bash\\ndocker exec -it elasticsearch bash\\n```\\n- Add path.repo configuration:\\n```bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart container and verify it was created correctly:\\n```bash\\ndocker restart elasticsearch\\ncurl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\\n```\\n- Create the snapshot (this is the backup ;) )\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Copy the backup to my machine:\\n```bash\\ndocker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\\n```\\n- Now create the new container or use docker-compose just in case you are following the module 2:\\n```bash\\ndocker compose up -d\\n```\\n- Add de path.repo configuration in the new one, same as before:\\n```bash\\ndocker exec -it new_elasticsearch bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart the docker container and copy the snapshot in it:\\n```bash\\ndocker restart new_elasticsearch\\ndocker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\\n```\\n- Register the Snapshot Repository in the New Container:\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"type\": \"fs\",\\n\"settings\": {\\n\"location\": \"/usr/share/elasticsearch/backup\"\\n}\\n}\\n\\'\\n```\\n- Verify if it exists:\\n```bash\\ncurl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\\n```\\n- Restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Show your indexes:\\n```bash\\ncurl -X GET \"localhost:9200/_cat/indices?v\"\\n```\\n- Extra point: If you want to change the original index name by other when you restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"old_index\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false,\\n\"rename_pattern\": \"old_index\",\\n\"rename_replacement\": \"new_index\"\\n}\\n\\'\\n```\\n',\n",
       "   'document': {'text': 'Yes, you can. Here the step to follow:\\n- Open a bash session in the elasticsearch container\\n```bash\\ndocker exec -it elasticsearch bash\\n```\\n- Add path.repo configuration:\\n```bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart container and verify it was created correctly:\\n```bash\\ndocker restart elasticsearch\\ncurl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\\n```\\n- Create the snapshot (this is the backup ;) )\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Copy the backup to my machine:\\n```bash\\ndocker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\\n```\\n- Now create the new container or use docker-compose just in case you are following the module 2:\\n```bash\\ndocker compose up -d\\n```\\n- Add de path.repo configuration in the new one, same as before:\\n```bash\\ndocker exec -it new_elasticsearch bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart the docker container and copy the snapshot in it:\\n```bash\\ndocker restart new_elasticsearch\\ndocker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\\n```\\n- Register the Snapshot Repository in the New Container:\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"type\": \"fs\",\\n\"settings\": {\\n\"location\": \"/usr/share/elasticsearch/backup\"\\n}\\n}\\n\\'\\n```\\n- Verify if it exists:\\n```bash\\ncurl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\\n```\\n- Restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Show your indexes:\\n```bash\\ncurl -X GET \"localhost:9200/_cat/indices?v\"\\n```\\n- Extra point: If you want to change the original index name by other when you restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"old_index\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false,\\n\"rename_pattern\": \"old_index\",\\n\"rename_replacement\": \"new_index\"\\n}\\n\\'\\n```',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:elasticsearch__can_i_backup_and_restore_my_elasticsearch_index_from_one_to_another_docker_container_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nElasticSearch: How can I limit the memory used by the ElasticSearch container?\\n\\nanswer:\\nYou can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\\n- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\\n- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\ncpus: \\'1.0\\'  # Limits to 1 CPU\\nreservations:\\ncpus: \\'0.5\\'  # Reserves 0.5 CPUs\\n',\n",
       "   'document': {'text': 'You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\\n- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\\n- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\ncpus: \\'1.0\\'  # Limits to 1 CPU\\nreservations:\\ncpus: \\'0.5\\'  # Reserves 0.5 CPUs',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: How can I limit the memory used by the ElasticSearch container?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:elasticsearch__how_can_i_limit_the_memory_used_by_the_elasticsearch_container_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nDocker: How to inspect the content of a file inside a Docker container ?\\n\\nanswer:\\nYou have several ways to inspect the content of a file when you are inside a Docker container.\\nFirst, make sure you ran the docker container interactively using bash:\\ndocker exec -it <container> bash\\nThen, you are able to use bash commands. For this case, I propose two solutions:\\nUse “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\\nInstall vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\\napt-get install vim\\nvim your_file\\nThen, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\\nAdded by Mélanie Fouesnard\\n',\n",
       "   'document': {'text': 'You have several ways to inspect the content of a file when you are inside a Docker container.\\nFirst, make sure you ran the docker container interactively using bash:\\ndocker exec -it <container> bash\\nThen, you are able to use bash commands. For this case, I propose two solutions:\\nUse “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\\nInstall vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\\napt-get install vim\\nvim your_file\\nThen, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How to inspect the content of a file inside a Docker container ?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:docker__how_to_inspect_the_content_of_a_file_inside_a_docker_container__'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nDocker: Error: Docker mounted volume adds ;C to end of windows path\\n\\nanswer:\\nUse the following line instead in mounting the current volume to docker for Q4:\\n`-v \"/${PWD}/ollama_files:/root/.ollama\"`\\n',\n",
       "   'document': {'text': 'Use the following line instead in mounting the current volume to docker for Q4:\\n`-v \"/${PWD}/ollama_files:/root/.ollama\"`',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Error: Docker mounted volume adds ;C to end of windows path'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:docker__error__docker_mounted_volume_adds__c_to_end_of_windows_path'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nDocker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?\\n\\nanswer:\\nIn Docker Desktop, try to increase the resource.\\nGo to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\\nAdded by Dandy Arif Rahman\\n',\n",
       "   'document': {'text': 'In Docker Desktop, try to increase the resource.\\nGo to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\\nAdded by Dandy Arif Rahman',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:docker__why_does_inferring_using_phi_3_locally_take_so_long_on_macbook_air_m1_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nDocker: How can to clean docker cache?\\n\\nanswer:\\ndocker system prune -a\\n',\n",
       "   'document': {'text': 'docker system prune -a',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How can to clean docker cache?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:docker__how_can_to_clean_docker_cache_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nOllama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama\\n\\nanswer:\\nA network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\\nAdded by Eduardo Muñoz\\n',\n",
       "   'document': {'text': 'A network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\\nAdded by Eduardo Muñoz',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:ollama___error__pull_model_manifest__503__no_healthy_upstream__when_pulling_a_model_with_ollama'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nOllama: Error: NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \"model XXX not found, try pulling it first\" …\\n\\nanswer:\\nTo solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\\nAdded by Taras Goriachko\\nOllama: Running Ollama locally on Colab gives error after the llm() line\\nAPIConnectionError: Connection error.\\nIt seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\\nFound a solution in the Medium article and this link:\\nhttps://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\\nhttps://github.com/ollama/ollama/issues/703\\nAdded by Hanaa\\n',\n",
       "   'document': {'text': 'To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\\nAdded by Taras Goriachko\\nOllama: Running Ollama locally on Colab gives error after the llm() line\\nAPIConnectionError: Connection error.\\nIt seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\\nFound a solution in the Medium article and this link:\\nhttps://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\\nhttps://github.com/ollama/ollama/issues/703\\nAdded by Hanaa',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error: NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \"model XXX not found, try pulling it first\" …'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:ollama__error__notfounderror__error_code__404_____error_____message____model_xxx_not_found__try_pulling_it_first___'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nOllama: How can remove Ollama model?\\n\\nanswer:\\nollama list\\nollama rm [model_name]\\n',\n",
       "   'document': {'text': 'ollama list\\nollama rm [model_name]',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: How can remove Ollama model?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:ollama__how_can_remove_ollama_model_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nOllama: Error code 500 InternalServerError\\n\\nanswer:\\nInternalServerError: Error code: 500 - {\\'error\\': {\\'message\\': \\'model requires more system memory (5.6 GiB) than is available (1.5 GiB)\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}.\\nRunning elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\\nversion: \\'3.8\\'\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\n- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\nmemory: 2G  # change 2\\nollama:\\nimage: ollama/ollama\\ncontainer_name: ollama\\nvolumes:\\n- ollama:/root/.ollama\\nports:\\n- \"11434:11434\"\\nvolumes:\\nollama:\\nAdded by Zoe Zelkha\\n',\n",
       "   'document': {'text': 'InternalServerError: Error code: 500 - {\\'error\\': {\\'message\\': \\'model requires more system memory (5.6 GiB) than is available (1.5 GiB)\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}.\\nRunning elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\\nversion: \\'3.8\\'\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\n- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\nmemory: 2G  # change 2\\nollama:\\nimage: ollama/ollama\\ncontainer_name: ollama\\nvolumes:\\n- ollama:/root/.ollama\\nports:\\n- \"11434:11434\"\\nvolumes:\\nollama:\\nAdded by Zoe Zelkha',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error code 500 InternalServerError'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:ollama__error_code_500_internalservererror'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nMistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF\\n\\nanswer:\\nManually set the token as below:\\naccess_token = <your_token>\\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\n',\n",
       "   'document': {'text': 'Manually set the token as below:\\naccess_token = <your_token>\\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:mistral_ai__unable_to_get_mistral_7b_v0_1_access_despite_accepting_terms_on_hf'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nPython: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\\n\\nanswer:\\nTo solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers\\n\",\n",
       "   'document': {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': \"Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\"},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:python__error__modulenotfounderror__no_module_named__transformers_cache_utils_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nPython: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3\\n\\nanswer:\\nTo solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers\\n',\n",
       "   'document': {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:python__exception__data_did_not_match_any_variant_of_untagged_enum_pypretokenizertypewrapper_at_line_40_column_3'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nPython: from google.protobuf.pyext import _message / TypeError: bases must be types\\n\\nanswer:\\npip install protobuf==3.20.1\\nAdded by Ibai Irastorza\\n',\n",
       "   'document': {'text': 'pip install protobuf==3.20.1\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: from google.protobuf.pyext import _message / TypeError: bases must be types'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:python__from_google_protobuf_pyext_import__message___typeerror__bases_must_be_types'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 2: Open-Source LLMs\\n\\nquestion:\\nHuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?\\n\\nanswer:\\n1. search with the model name on hugging face.\\n2. get the transformer used on the model.\\n3. using the transformer, encode the string you want.\\n4. calculate the length of the outputted tensor.\\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM. \\nDon’t forget to make your token secret.\\nAdded by kamal\\n',\n",
       "   'document': {'text': '1. search with the model name on hugging face.\\n2. get the transformer used on the model.\\n3. using the transformer, encode the string you want.\\n4. calculate the length of the outputted tensor.\\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM. \\nDon’t forget to make your token secret.\\nAdded by kamal',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?'},\n",
       "   'document_id': 'llm_zoomcamp:module_2__open_source_llms:huggingface__how_to_get_the_number_of_tokens_in_a_certain_string_related_to_a_certain_model_on_hugging_face_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nHow to run a model using CUDA for GPU usage?\\n\\nanswer:\\nThe last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\\nIn my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\\nTo install all the needed packages use this command:\\n!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\\nAnd after that just executed this command:\\n!pip install --upgrade transformers\\n',\n",
       "   'document': {'text': 'The last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\\nIn my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\\nTo install all the needed packages use this command:\\n!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\\nAnd after that just executed this command:\\n!pip install --upgrade transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'How to run a model using CUDA for GPU usage?'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:how_to_run_a_model_using_cuda_for_gpu_usage_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument \\'document\\'\\n\\nanswer:\\nUpgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\\nOr use the keyword ‘body’ instead of ‘document’\\nFor conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\\n\\n$ conda config --add channels conda-forge\\n$ conda config --set channel_priority strict\\n$ conda install -c conda-forge elasticsearch==8.4.3\\n',\n",
       "   'document': {'text': 'Upgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\\nOr use the keyword ‘body’ instead of ‘document’\\nFor conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\\n\\n$ conda config --add channels conda-forge\\n$ conda config --set channel_priority strict\\n$ conda install -c conda-forge elasticsearch==8.4.3',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:elasticsearch__error__elasticsearch_index___got_an_unexpected_keyword_argument__document_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\\n\\nanswer:\\nThis worked for me:\\n\",\n",
       "   'document': {'text': 'This worked for me:',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\"},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:elasticsearch__typeerror__elasticsearch_search___got_an_unexpected_keyword_argument__knn_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search\\n\\nanswer:\\nTry to running docker container based on first course module like this :\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnd don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode\\n',\n",
       "   'document': {'text': 'Try to running docker container based on first course module like this :\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnd don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'ElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:elasticsearch__connectionerror__connection_error_caused_by__connectionerror_connection_error_caused_by__newconnectionerror__urllib3_connection_httpconnection_object_at_0x7c455bb94ac0___failed_to_establish_a_new_connection___errno_111__connection_refused___in_elastic_search'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nWhy do I get scores greater than 1 on my hits after querying my ElasticSearch database ?\\n\\nanswer:\\nAs seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\\nThe KNN related score, which is between -1 and 1 (cosine similarity)\\nThe text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document\\'s relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\\nSince we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\\nTo get more details about the final scores, you can modify the search query and add an “explain” parameter:\\nresponse = es_client.search(\\nindex=index_name,\\nquery={\\n\"match\": {\"section\": \"General course-related questions\"},\\n},\\nknn=knn_query,\\nsize=5,\\nexplain=True\\n)\\nAdded by Mélanie Fouesnard\\n',\n",
       "   'document': {'text': 'As seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\\nThe KNN related score, which is between -1 and 1 (cosine similarity)\\nThe text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document\\'s relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\\nSince we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\\nTo get more details about the final scores, you can modify the search query and add an “explain” parameter:\\nresponse = es_client.search(\\nindex=index_name,\\nquery={\\n\"match\": {\"section\": \"General course-related questions\"},\\n},\\nknn=knn_query,\\nsize=5,\\nexplain=True\\n)\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Why do I get scores greater than 1 on my hits after querying my ElasticSearch database ?'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:why_do_i_get_scores_greater_than_1_on_my_hits_after_querying_my_elasticsearch_database__'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nNot module named “sentence_transformers”\\n\\nanswer:\\nFor this module homework make sure you install the package sentence-transformers it can be installed as simply as:\\npip install sentence-transformers\\n',\n",
       "   'document': {'text': 'For this module homework make sure you install the package sentence-transformers it can be installed as simply as:\\npip install sentence-transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Not module named “sentence_transformers”'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:not_module_named__sentence_transformers_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nCan not create the index: Connection timeout.\\n\\nanswer:\\nI was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\\nI checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\\nMy problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\\ndocker image prune\\ndocker volume prune\\ndocker container prune\\nAdded by Ibai Irastorza\\n',\n",
       "   'document': {'text': 'I was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\\nI checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\\nMy problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\\ndocker image prune\\ndocker volume prune\\ndocker container prune\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Can not create the index: Connection timeout.'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:can_not_create_the_index__connection_timeout_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nTypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\\n\\nanswer:\\nMake sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\\nv_query = embedding_model.encode(query_text)\\nresults = search_function(v_query)\\n\",\n",
       "   'document': {'text': 'Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\\nv_query = embedding_model.encode(query_text)\\nresults = search_function(v_query)',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\"},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:typeerror__unsupported_operand_type_s__for_____float__and__dict__when_running_the_vector_search_function_within_the_evaluate_function'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nFind maximum of an numpy array (of any dimension):\\n\\nanswer:\\nmax_value = numpy_array.max()\\n',\n",
       "   'document': {'text': 'max_value = numpy_array.max()',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Find maximum of an numpy array (of any dimension):'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:find_maximum_of_an_numpy_array__of_any_dimension__'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nWhat is the cosine similarity?\\n\\nanswer:\\nCosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.\\n',\n",
       "   'document': {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What is the cosine similarity?'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:what_is_the_cosine_similarity_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 3: X\\n\\nquestion:\\nWhat are documents in ElasticSearch?\\n\\nanswer:\\nA “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.\\n',\n",
       "   'document': {'text': 'A “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What are documents in ElasticSearch?'},\n",
       "   'document_id': 'llm_zoomcamp:module_3__x:what_are_documents_in_elasticsearch_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 4: Monitoring\\n\\nquestion:\\nruning docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See \\'docker run --help\\'.\\n\\nanswer:\\ndocker stop elasticsearch\\ndocker rm elasticsearch\\nHow to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\\nscore = (es_score - 0.5) * 2\\n',\n",
       "   'document': {'text': 'docker stop elasticsearch\\ndocker rm elasticsearch\\nHow to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\\nscore = (es_score - 0.5) * 2',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'runing docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See \\'docker run --help\\'.'},\n",
       "   'document_id': 'llm_zoomcamp:module_4__monitoring:runing_docker_docker__error_response_from_daemon__conflict__the_container_name___elasticsearch__is_already_in_use_by_container__20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665___you_have_to_remove__or_rename__that_container_to_be_able_to_reuse_that_name__see__docker_run___help__'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 4: Monitoring\\n\\nquestion:\\nWarning: \\'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet\\' how to suppress?\\n\\nanswer:\\nUpgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings\\n',\n",
       "   'document': {'text': 'Upgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'Warning: \\'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet\\' how to suppress?'},\n",
       "   'document_id': 'llm_zoomcamp:module_4__monitoring:warning___model__multi_qa_mpnet_base_dot_v1__was_made_on_sentence_transformers_v3_0_0_bet__how_to_suppress_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 4: Monitoring\\n\\nquestion:\\nIn Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\lib\\\\fbgemm.dll\" or one of its dependencies.\\n\\nanswer:\\nSolution 1 : Install Visual C++ Redistributable\\nSolution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969\\n',\n",
       "   'document': {'text': 'Solution 1 : Install Visual C++ Redistributable\\nSolution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'In Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\lib\\\\fbgemm.dll\" or one of its dependencies.'},\n",
       "   'document_id': 'llm_zoomcamp:module_4__monitoring:in_windows_os___oserror___winerror_126__the_specified_module_could_not_be_found__error_loading__c__users_user_appdata_local_programs_python_python310_lib_site_packages_torch_lib_fbgemm_dll__or_one_of_its_dependencies_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 4: Monitoring\\n\\nquestion:\\nOperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?\\n\\nanswer:\\nInside .env file change POSTGRES_HOST=localhost\\n',\n",
       "   'document': {'text': 'Inside .env file change POSTGRES_HOST=localhost',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?'},\n",
       "   'document_id': 'llm_zoomcamp:module_4__monitoring:operationalerror_when_running_python_prep_pypsycopg2__operationalerror__could_not_translate_host_name__postgres__to_address__no_such_host_is_known__how_do_i_fix_this_issue_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 4: Monitoring\\n\\nquestion:\\nHow set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook\\n\\nanswer:\\nBy default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\\nThe instruction to show the results must be preceded by:\\npd.set_option(\\'display.max_colwidth\\', None)\\nHere are the specs for the display_max_colwidth option, as describide in the official docs:\\ndisplay.max_colwidth : int or None\\nThe maximum width in characters of a column in the repr of\\na pandas data structure. When the column overflows, a \"...\"\\nplaceholder is embedded in the output. A \\'None\\' value means unlimited.\\n[default: 50] [currently: 50]\\n',\n",
       "   'document': {'text': 'By default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\\nThe instruction to show the results must be preceded by:\\npd.set_option(\\'display.max_colwidth\\', None)\\nHere are the specs for the display_max_colwidth option, as describide in the official docs:\\ndisplay.max_colwidth : int or None\\nThe maximum width in characters of a column in the repr of\\na pandas data structure. When the column overflows, a \"...\"\\nplaceholder is embedded in the output. A \\'None\\' value means unlimited.\\n[default: 50] [currently: 50]',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook'},\n",
       "   'document_id': 'llm_zoomcamp:module_4__monitoring:how_set_pandas_to_show_entire_text_content_in_a_column__useful_to_view_the_entire_explanation_column_content_in_the_llm_as_judge_section_of_the_offline_rag_evaluation_notebook'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 4: Monitoring\\n\\nquestion:\\nHow to normalize vectors in a Pandas DataFrame column (or Pandas Series)?\\n\\nanswer:\\nimport numpy as np\\nnormalize_vec = lambda v: v / np.linalg.norm(v)\\ndf[\"new_col\"] = df[\"org_col\"].apply(norm_vec)\\n',\n",
       "   'document': {'text': 'import numpy as np\\nnormalize_vec = lambda v: v / np.linalg.norm(v)\\ndf[\"new_col\"] = df[\"org_col\"].apply(norm_vec)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?'},\n",
       "   'document_id': 'llm_zoomcamp:module_4__monitoring:how_to_normalize_vectors_in_a_pandas_dataframe_column__or_pandas_series__'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 4: Monitoring\\n\\nquestion:\\nHow to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?\\n\\nanswer:\\nTo compute the 75% percentile or 0.75 quantile:\\nquantile: int = df[\"col\"].quantile(q=0.75)\\n',\n",
       "   'document': {'text': 'To compute the 75% percentile or 0.75 quantile:\\nquantile: int = df[\"col\"].quantile(q=0.75)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?'},\n",
       "   'document_id': 'llm_zoomcamp:module_4__monitoring:how_to_compute_the_quantile_or_percentile_of_pandas_dataframe_column__or_pandas_series__'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 5: X\\n\\nquestion:\\nHow can I remove all Docker containers, images, and volumes, and builds from the terminal?\\n\\nanswer:\\n1. Delete all containers (including running ones):\\n```\\ndocker rm -f\\n```\\n2. Remove all images:\\n```\\ndocker rmi -f\\n```\\n3. Delete all volumes:\\n```\\ndocker volume rm\\n```\\n',\n",
       "   'document': {'text': '1. Delete all containers (including running ones):\\n```\\ndocker rm -f\\n```\\n2. Remove all images:\\n```\\ndocker rmi -f\\n```\\n3. Delete all volumes:\\n```\\ndocker volume rm\\n```',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': 'How can I remove all Docker containers, images, and volumes, and builds from the terminal?'},\n",
       "   'document_id': 'llm_zoomcamp:module_5__x:how_can_i_remove_all_docker_containers__images__and_volumes__and_builds_from_the_terminal_'},\n",
       "  {'chunk': \"course:\\nllm-zoomcamp\\n\\nsection:\\nModule 5: X\\n\\nquestion:\\nI have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\\n\\nanswer:\\nUse the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>\\n\",\n",
       "   'document': {'text': 'Use the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': \"I have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\"},\n",
       "   'document_id': 'llm_zoomcamp:module_5__x:i_have_reached_the_orchestration_pipeline_s_export_and_i_m_facing_a_connection_error_at_the_stage_of_exporting_to_the_vector_database__can_someone_help_with_the_connection_string_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 6: X\\n\\nquestion:\\nQuestion\\n\\nanswer:\\nAnswer\\n',\n",
       "   'document': {'text': 'Answer',\n",
       "    'section': 'Module 6: X',\n",
       "    'question': 'Question'},\n",
       "   'document_id': 'llm_zoomcamp:module_6__x:question'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nModule 6: X\\n\\nquestion:\\nQuestion\\n\\nanswer:\\nAnswer\\n',\n",
       "   'document': {'text': 'Answer',\n",
       "    'section': 'Module 6: X',\n",
       "    'question': 'Question'},\n",
       "   'document_id': 'llm_zoomcamp:module_6__x:question'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nCapstone Project\\n\\nquestion:\\nQuestion\\n\\nanswer:\\nAnswer\\n',\n",
       "   'document': {'text': 'Answer',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Question'},\n",
       "   'document_id': 'llm_zoomcamp:capstone_project:question'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nCapstone Project\\n\\nquestion:\\nIs it a group project?\\n\\nanswer:\\nNo, the capstone is a solo project.\\n',\n",
       "   'document': {'text': 'No, the capstone is a solo project.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Is it a group project?'},\n",
       "   'document_id': 'llm_zoomcamp:capstone_project:is_it_a_group_project_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nCapstone Project\\n\\nquestion:\\nDo we submit 2 projects, what does attempt 1 and 2 mean?\\n\\nanswer:\\nYou only need to submit 1 project. \\nIf the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\\nIf you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\\nIf you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort\\n',\n",
       "   'document': {'text': 'You only need to submit 1 project. \\nIf the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\\nIf you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\\nIf you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Do we submit 2 projects, what does attempt 1 and 2 mean?'},\n",
       "   'document_id': 'llm_zoomcamp:capstone_project:do_we_submit_2_projects__what_does_attempt_1_and_2_mean_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nCapstone Project\\n\\nquestion:\\nDoes the competition count as the capstone?\\n\\nanswer:\\nNo, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.\\n',\n",
       "   'document': {'text': 'No, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Does the competition count as the capstone?'},\n",
       "   'document_id': 'llm_zoomcamp:capstone_project:does_the_competition_count_as_the_capstone_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nCapstone Project\\n\\nquestion:\\nHow is my capstone project going to be evaluated?\\n\\nanswer:\\nEach submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).\\n',\n",
       "   'document': {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'How is my capstone project going to be evaluated?'},\n",
       "   'document_id': 'llm_zoomcamp:capstone_project:how_is_my_capstone_project_going_to_be_evaluated_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nCertificates\\n\\nquestion:\\nDo I have to use ElasticSearch or X library?\\n\\nanswer:\\nAnswer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.\\n',\n",
       "   'document': {'text': 'Answer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.',\n",
       "    'section': 'Certificates',\n",
       "    'question': 'Do I have to use ElasticSearch or X library?'},\n",
       "   'document_id': 'llm_zoomcamp:certificates:do_i_have_to_use_elasticsearch_or_x_library_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: dlthub\\n\\nquestion:\\nQuestion\\n\\nanswer:\\nAnswer\\n',\n",
       "   'document': {'text': 'Answer',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Question'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__dlthub:question'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: dlthub\\n\\nquestion:\\nCan I use the workshop materials for my own projects or share them with others?\\n\\nanswer:\\nSince dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\\nAdded by Mélanie Fouesnard\\n',\n",
       "   'document': {'text': 'Since dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Can I use the workshop materials for my own projects or share them with others?'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__dlthub:can_i_use_the_workshop_materials_for_my_own_projects_or_share_them_with_others_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: dlthub\\n\\nquestion:\\nThere is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\\n\\nanswer:\\nThe error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings\\n',\n",
       "   'document': {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__dlthub:there_is_an_error_when_opening_the_table_using_dbtable___db_open_table__notion_pages___homework____filenotfounderror__table_notion_pages___homework_does_not_exist_please_first_call_db_create_table_notion_pages___homework__data_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: dlthub\\n\\nquestion:\\nThere is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\\n\\nanswer:\\nMake sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")\\n',\n",
       "   'document': {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__dlthub:there_is_an_error_when_running_main____filenotfounderror__table_notion_pages___homework_does_not_exist_please_first_call_db_create_table_notion_pages___homework__data_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: dlthub\\n\\nquestion:\\nHow do I know which tables are in the db\\n\\nanswer:\\nYou can use the db.table_names() to list all the tables in the db\\n',\n",
       "   'document': {'text': 'You can use the db.table_names() to list all the tables in the db',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'How do I know which tables are in the db'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__dlthub:how_do_i_know_which_tables_are_in_the_db'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: dlthub\\n\\nquestion:\\nDoes DLT have connectors to ClickHouse or StarRocks?\\n\\nanswer:\\nCurrently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.\\n',\n",
       "   'document': {'text': 'Currently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Does DLT have connectors to ClickHouse or StarRocks?'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__dlthub:does_dlt_have_connectors_to_clickhouse_or_starrocks_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: dlthub\\n\\nquestion:\\nNotebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search\\n\\nanswer:\\nIf you get this error\\nOr 401 Client Error , then you either need to grant access to the key or the key is wrong.\\n',\n",
       "   'document': {'text': 'If you get this error\\nOr 401 Client Error , then you either need to grant access to the key or the key is wrong.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Notebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__dlthub:notebook_does_not_have_secret_access_or_401_client_error__unauthorized_for_url__https___api_notion_com_v1_search'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: X\\n\\nquestion:\\nError: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?\\n\\nanswer:\\nInstall directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`\\n',\n",
       "   'document': {'text': 'Install directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__x:error__how_to_fix_requests_library_only_installs_v2_28_instead_of_v2_32_required_for_lancedb_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: X\\n\\nquestion:\\nConnection refused error on prompting the ollam RAG?\\n\\nanswer:\\nIf you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\\nIf you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\\nAdded by Abiodun Gbadamosi\\n',\n",
       "   'document': {'text': 'If you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\\nIf you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\\nAdded by Abiodun Gbadamosi',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Connection refused error on prompting the ollam RAG?'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__x:connection_refused_error_on_prompting_the_ollam_rag_'},\n",
       "  {'chunk': 'course:\\nllm-zoomcamp\\n\\nsection:\\nWorkshops: X\\n\\nquestion:\\nQuestion\\n\\nanswer:\\nAnswer\\n',\n",
       "   'document': {'text': 'Answer',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Question'},\n",
       "   'document_id': 'llm_zoomcamp:workshops__x:question'}]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cc8f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "#@data_exporter\n",
    "def elasticsearch(documents: List[Dict[str, Union[Dict, List[int], str]]], *args, **kwargs):\n",
    "    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n",
    "    #index_name = kwargs.get('index_name', 'documents')\n",
    "    index_name_prefix = kwargs.get('index_name', 'documents')\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index name:\", index_name)\n",
    "    number_of_shards = kwargs.get('number_of_shards', 1)\n",
    "    number_of_replicas = kwargs.get('number_of_replicas', 0)\n",
    "    dimensions = kwargs.get('dimensions')\n",
    "\n",
    "    if dimensions is None and len(documents) > 0:\n",
    "        document = documents[0]\n",
    "        dimensions = len(document.get('embedding') or [])\n",
    "\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "\n",
    "    print(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": number_of_shards,\n",
    "            \"number_of_replicas\": number_of_replicas,\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"chunk\": {\"type\": \"text\"},\n",
    "                \"document_id\": {\"type\": \"text\"},\n",
    "                \"embedding\": {\"type\": \"dense_vector\", \"dims\": dimensions}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Recreate the index by deleting if it exists and then creating with new settings\n",
    "    if es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.delete(index=index_name)\n",
    "        print(f'Index {index_name} deleted')\n",
    "\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "    print('Index created with properties:')\n",
    "    print(json.dumps(index_settings, indent=2))\n",
    "    print('Embedding dimensions:', dimensions)\n",
    "\n",
    "    count = len(documents)\n",
    "    print(f'Indexing {count} documents to Elasticsearch index {index_name}')\n",
    "    for idx, document in enumerate(documents):\n",
    "        if idx % 100 == 0:\n",
    "\t\t        print(f'{idx + 1}/{count}')\n",
    "\n",
    "        if isinstance(document['embedding'], np.ndarray):\n",
    "            document['embedding'] = document['embedding'].tolist()\n",
    "\n",
    "        es_client.index(index=index_name, document=document)\n",
    "\n",
    "    return [[d['embedding'] for d in documents[:10]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6bfcc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index name: documents_20240822_5858\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m doc_exp \u001b[38;5;241m=\u001b[39m \u001b[43melasticsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_doc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 22\u001b[0m, in \u001b[0;36melasticsearch\u001b[0;34m(documents, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimensions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dimensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(documents) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m     dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(document\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m     25\u001b[0m es_client \u001b[38;5;241m=\u001b[39m Elasticsearch(connection_string)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "doc_exp = elasticsearch(dict_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
